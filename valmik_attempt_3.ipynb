{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9193f28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import csv\n",
    "\n",
    "# Basic settings\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = ImageFolder(root=\"2/Training\", transform=transform)\n",
    "test_dataset = ImageFolder(root=\"2/Testing\", transform=transform)\n",
    "\n",
    "# Split training into train and validation\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_set, val_set = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "num_classes = len(train_dataset.classes)\n",
    "\n",
    "# Modified Model Definition\n",
    "class CustomResNet(nn.Module):\n",
    "    def __init__(self, n_layers, width, dropout_rate, num_classes):\n",
    "        super(CustomResNet, self).__init__()\n",
    "        self.features = self._make_layers(n_layers, width, dropout_rate)\n",
    "        self.classifier = nn.Linear(width, num_classes)\n",
    "    \n",
    "    def _make_layers(self, n_layers, width, dropout_rate):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for i in range(n_layers):\n",
    "            layers.extend([\n",
    "                nn.Conv2d(in_channels, width, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(width),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout2d(p=dropout_rate)\n",
    "            ])\n",
    "            in_channels = width\n",
    "        # Final adaptive pooling\n",
    "        layers.append(nn.AdaptiveAvgPool2d((1, 1)))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Metrics (unchanged)\n",
    "def calculate_metrics(y_true, y_pred, num_classes):\n",
    "    precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(num_classes)))\n",
    "    label_counts = cm.sum(axis=0)\n",
    "    bias = np.std(label_counts) / (np.mean(label_counts) + 1e-6)\n",
    "    return precision, recall, bias\n",
    "\n",
    "# Modified objective function with new n_layers range\n",
    "def objective(trial):\n",
    "    n_layers = trial.suggest_int('n_layers', 10, 50)  # Modified range\n",
    "    width = trial.suggest_int('width', 50, 500)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.5)\n",
    "\n",
    "    model = CustomResNet(n_layers, width, dropout_rate, num_classes).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(3):\n",
    "        model.train()\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_labels.append(targets.cpu().numpy())\n",
    "\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    precision, recall, bias = calculate_metrics(all_labels, all_preds, num_classes)\n",
    "    return precision + recall - bias\n",
    "\n",
    "# Rest of the code remains the same\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "# New functionality: Store all points and Pareto frontier\n",
    "def is_dominated(point, other):\n",
    "    \"\"\"Check if a point is dominated by another point\"\"\"\n",
    "    return (other[3] >= point[3] and  # precision\n",
    "            other[4] >= point[4] and  # recall\n",
    "            other[5] <= point[5] and   # bias\n",
    "            (other[3] > point[3] or other[4] > point[4] or other[5] < point[5]))\n",
    "\n",
    "def get_pareto_frontier(results):\n",
    "    \"\"\"Identify Pareto-optimal points\"\"\"\n",
    "    frontier = []\n",
    "    for i, point in enumerate(results):\n",
    "        dominated = False\n",
    "        for j, other in enumerate(results):\n",
    "            if i != j and is_dominated(point, other):\n",
    "                dominated = True\n",
    "                break\n",
    "        if not dominated:\n",
    "            frontier.append(point)\n",
    "    return frontier\n",
    "\n",
    "# Collect results with hyperparameters\n",
    "results = []\n",
    "best_trials = sorted(study.trials, key=lambda t: t.value, reverse=True)[:5]\n",
    "\n",
    "for trial in best_trials:\n",
    "    params = trial.params\n",
    "    model = CustomResNet(**params, num_classes=num_classes).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Training (unchanged)\n",
    "    for epoch in range(5):\n",
    "        model.train()\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation (unchanged)\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_labels.append(targets.cpu().numpy())\n",
    "\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    precision, recall, bias = calculate_metrics(all_labels, all_preds, num_classes)\n",
    "    \n",
    "    # Store with hyperparameters and metrics\n",
    "    results.append((\n",
    "        params['n_layers'],\n",
    "        params['width'],\n",
    "        params['dropout_rate'],\n",
    "        precision,\n",
    "        recall,\n",
    "        bias\n",
    "    ))\n",
    "\n",
    "# Save all points to CSV\n",
    "with open('attempt_2_all_points.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['n_layers', 'width', 'dropout', 'precision', 'recall', 'bias'])\n",
    "    writer.writerows(results)\n",
    "\n",
    "# Get and save Pareto frontier\n",
    "pareto_front = get_pareto_frontier(results)\n",
    "with open('attempt_2_pareto_points.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['n_layers', 'width', 'dropout', 'precision', 'recall', 'bias'])\n",
    "    writer.writerows(pareto_front)\n",
    "\n",
    "# Visualization (unchanged)\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "precisions = [r[3] for r in pareto_front]\n",
    "recalls = [r[4] for r in pareto_front]\n",
    "biases = [r[5] for r in pareto_front]\n",
    "ax.scatter(precisions, recalls, biases, c='r', marker='o')\n",
    "ax.set_xlabel('Precision')\n",
    "ax.set_ylabel('Recall')\n",
    "ax.set_zlabel('Bias')\n",
    "plt.title('Pareto Frontier for Multi-objective Model')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mooenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
